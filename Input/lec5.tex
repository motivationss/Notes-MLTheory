\part{Optimization}


\chapter{Gradient Descent}
Please refer to appendix 1 and 2 for a basic calculus and linear algebra recap at first. 



Gradient Descent is an iterative algorithm, where we are concerned with 
\[
   \text{min}_{x \in \R^d} f(x)  
\]

The algorithm starts at \(x_0 \in \R^d\) and iteratively update the variable 
\(x_1, x_2, \ldots\). 

When the point is at \(x_t\), we can do 1st-order Taylor expansion: 

\[
    f(x_t + \Delta x) \approx f(x_t) + \innerProd{\nabla f(x_t)}{ \Delta x} + \cdots 
\]


In order to decrease \(f\) as much as possible, we can choose \(\Delta x // - \nabla f(x_t)\). 

\begin{remark}
    \[
        \inf_{\norm{\Delta x}_2 \leq \epsilon} \innerProd{a}{\Delta x} = - \epsilon \norm{a}_2  
    \]
    the optimum occurs at \(\Delta x = \epsilon \frac{a}{\norm{a}_2}\)
\end{remark}


This motivates \textbf{Gradient Descent}(GD). 

\[
    x_{t+1} = x_t - \eta \nabla f(x_t), \ t = 0,1,2,\ldots
\]

where \(\eta > 0\) is called \emph{step size} or \emph{learning rate}. 

In order for GD to do what it's supposed to do, we want the 1st-order Taylor expansion to be 
accurate. 

Error of 1st-order Taylor: 


\begin{align*}
    f(x) - f(x_t) -  \innerProd{\nabla f(x_t)}{x - x_t} &= \frac{1}{2} (x - x_t)^T \nabla^2 f(\xi) (x - x_t) \\ 
    & \leq \frac{1}{2} \lVert \nabla^2 f(\xi)\rVert_2 + \lVert x - x_t\rVert_2^2
\end{align*}

\begin{definition}[smoothness]
    A differentiable function \(f \colon \R^d \to \R\) is \(\beta\)-smooth (\(\beta > 0\)) if 
    \[
        \lVert \nabla f(x) - \nabla f(y)\rVert_2 \leq \beta\lVert x - y\rVert_2, \ \forall \ x, y   
    \]
    In other words, gradient of \(f\) is \(\beta\)-Lipschitz. 
\end{definition}

\begin{remark}
    When \(f\) is twice differentiable, \(f \colon \R^d \to \R\) is equivalent to 
    \[
        \lVert \nabla^2 f(x)\rVert_2 \leq \beta, \ \forall \ x   
    \]
\end{remark}

\begin{lemma}
    If \(f\) is \(\beta\)-smooth, then: 
    \[
        \absolute{ f(y) - f(x) - \innerProd{\nabla f(x)}{y - x}}  \leq \frac{\beta}{2} \lVert x - y\rVert_2^2  
    \]
\end{lemma}


\begin{proof}
    \begin{align*}
        \absolute{f(y) - f(x) - \innerProd{\nabla f(x)}{ y - x }} 
        &= \absolute{ \int_0^1 \innerProd{\nabla f(x + t(y-x))}{y - x} - \int_0^1 \innerProd{\nabla f(x)}{y-x}dt} \tag*{FTC} \\ 
        &= \absolute{\int_0^1 \innerProd{\nabla f(x + t(y-x)) - \nabla f(x)}{y-x}dt} \\ 
        &\leq \int_0^1 \norm{\nabla f(x + t(y-x)) - \nabla f(x)}_2 \norm{y-x}_2 \tag*{Cauchy-Schwardz} \\ 
        &\leq \int_0^1 \beta \cdot \norm{t(y-x)}_2 \cdot \norm{y-x}_2 dt \tag*{beta - smooth}  \\ 
        &= \beta \norm{y-x}_2^2 \cdot \int_0^1 t dt \\ 
        &= \frac{\beta}{2} \norm{y-x}_2^2 
    \end{align*}
\end{proof}


\begin{lemma}[Descent Lemma]
    If \(f\) is \(\beta\)-smooth and \(\eta \leq \frac{1}{\beta}\), then GD with 
    step size \(\eta\) (\(x_{t+1} = x_t - \eta \nabla f(x_t)\)) satisfies 
    \[
        f(x_{t+1}) \leq f(x_t) - \frac{\eta}{2} \norm{\nabla f(x_t)}_2^2 
    \]
\end{lemma}

\begin{proof}
    \begin{align*}
        f(x_{t+1}) &\leq f(x_t) + \innerProd{f(x_t)}{t_1 - t} + \frac{\beta}{2} \norm{t_1 - t}_2^2  \tag*{\text{previous lemma}} \\ 
        &=f(x_t) + \innerProd{f(x_t)}{- \eta \nabla f(x_t)} + \frac{\beta}{2} \norm{- \eta \nabla f(x_t)}_2^2 \\ 
        &= f(x_t) - \left( \eta - \frac{\beta}{2} \eta^2 \right) \norm{f(x_t)}_2^2 \\
        &\leq f(x_t) - \frac{\eta}{2} \norm{f(x_t)}_2^2
    \end{align*}
\end{proof}


\begin{remark}
    Descent Lemma shows that every step in a \(\beta-\)smooth function \(f\) decreases 
    the function value. 
\end{remark}


\begin{corollary}
    If \(f\) is \(\beta\)-smooth, then GD with step size \(\eta \leq \frac{1}{\beta}\) must 
    satisfy: 
    \begin{itemize}
        \item \(\lim_{t \to \infty} f(x_t)\) exists 
        \item \(\lim_{t \to \infty} \norm{\nabla f(x_t)}_2 = 0\), since function converges and 
        \(f(x_t) - f(x)\) is bounded by it. 
    \end{itemize}
\end{corollary}

\section{Convex Optimization}

\begin{definition}[convexity] We present the following definitions: 
    
    \textbf{convex set}: A set \(X \subseteq \R^d\) is convex if 
    \[
        \forall x, y \in X, \ \forall \gamma \in (0,1) \colon (1 - \gamma)x + \gamma y \in X   
    \]

    \textbf{convex function}: A function \(f \colon X \to \R\) is convex if \(X\) is convex 
    and 
    \[
        \forall x, y \in X, \ \forall \gamma \in (0,1) \colon 
        f \left((1 - \gamma)x + \gamma y\right) \leq (1 - \gamma) f(x) + \gamma f(y)
    \]
\end{definition}

\begin{eg} Common Convex Functions 

    \begin{itemize}
        \item linear function 
        \item squared norm
    \end{itemize}
    
\end{eg}

\begin{eg}[Examples that preserve convexity] E.g. 


    \begin{itemize}
        \item non-negative weighted sum 
        \item composition with affine mapping 
        \item pointwise supreme 
    \end{itemize}
\end{eg}


\begin{eg}[Linear Model] Given dataset \(S = \{x_i, y_i\}_{i=1}^n\), \(\Hcal = \{x \mapsto w^T x \colon 
    w \in \R^d\}\). Empirical risk \(L_S(w) = \frac{1}{n} \sum_{i=1}^n \ell (w^Tx_i, y_i)\)

    \textbf{claim:} If \(\ell (y', y)\) is convex in its first argument (for any fixed \(y\)), 
    then \(L_S\) is convex. 
\end{eg}

Let's see the common loss functions that are convex in first argument. (\(y \in \{\pm 1\}\))

\begin{itemize}
    \item squared loss: \(\ell (y', y) = (y - y')^2\) Convex 
    \item 0-1 loss: \(\ell(y', y) = \indi \midBrac{yy' \leq 0}\) not Convex 
    \item Margine loss: not convex 
    \item Hinge loss: convex 
    \item logistic loss: \(\ell (y', y) = \log (1 + e^{-yy'})\) convex 
\end{itemize}

\begin{lemma}[first-order \& second-order characterization of convex functions]
    First, if \(f\) is differentiable, then \(f\) is convex if and only if 
    \[
        f(y) \geq f(x) + \innerProd{\nabla f(x)}{y - x}, \ \forall x, y  
    \]

    Second, if \(f\) is twice-continuously differentiable, then \(f\) is convex if and only if 
    \[
        \nabla^2 f(x) \succeq 0, \ \forall x   
    \]
\end{lemma}

\begin{definition}[Local Minimum] 
    A local minimum of a function \(f \colon \R^d \to \R\) is a point \(x \in \R\) such that 
    \(\exists  \ \epsilon > 0\): 
    \[
        f(x) \leq f(y), \ \forall \ y \text{ satisfying } \norm{y-x}_2 \leq \epsilon
    \]
\end{definition}


\begin{lemma}
    Every local minimum of a convex function is a global minimum. 
\end{lemma}
\begin{proof}
    Suppose \(x\) is a local minimum but not global minimum, \emph{i.e.},  
    \(\exists \ y\) s.t. \(f(y) < f(x)\). 

    By convexity, for all \(\gamma \in (0, 1)\), 
    \begin{align*}
        f( (1- \gamma)x + \gamma y) 
        &\leq (1 - \gamma) f(x) + \gamma f(y) \\ 
        &\leq (1 - \gamma) f(x) + \gamma f(x) \\ 
        &= f(x)
    \end{align*}

    As we take \(\gamma \to 0\), we have \(\norm{(1- \gamma) x + \gamma y - x}_2 \to 0\), 
    yielding a contraction. 
\end{proof}

\begin{remark}
    \color{red} Isn't it "As we take \(\gamma \to 1\), we have \(\norm{f\left((1- \gamma) x + \gamma y\right) - f(x)}_2 \to 0\), 
    yielding a contraction."
\end{remark}

\begin{lemma}
    If \(f \colon \R^d \to \R\) is convex and differentiable, and \(\nabla f(x) = 0\) 
    (\emph{i.e.} \(x\) is a stationary point), then \(x\) is a global minimum. 
\end{lemma}

\begin{proof}
    \(\forall \ y, f(y) \geq f(x) + \innerProd{\nabla f(x)}{y - x} = f(x)\)
\end{proof}

\begin{lemma}
    If \(f \colon \R^d \to \R\) is differentiable, and \(x\) is a local minimum, then 
    \(\nabla f(x) = 0\). 
\end{lemma} 


\begin{corollary}
    If \(f\) is convex and differentiable, then \(x\) is a global minimum if and only 
    if \(\nabla f(x) = 0\). 
\end{corollary}


\section{Convergence of GD for Smooth Convex Functions}



\begin{lemma}[contraction lemma] 
    If \(f\) is convex and \(\beta\)-smooth, and \(\eta \leq \frac{1}{\beta}\), then: 

    \[
        \norm{x_{t+1} - x^*}_2 \leq \norm{x_t - x^*}_2, \ \forall \ t   
    \]
\end{lemma}

\begin{proof}
    \begin{align*}
        \norm{x_{t+1} - x^*}^2_2 &= \norm{(x_{t} - x^*) - \eta \nabla f(x)}_2^2 \\ 
        &= \norm{x_{t} - x^*}_2^2 - 2 \eta \innerProd{x_t - x^*}{\nabla f(x_t)} + 
        \eta^2 \norm{\nabla f(x)}_2^2 \\ 
        &\leq \norm{ x_t - x^* }_2^2 - 2 \eta \left(f(x_t) - f(x^*)\right) + 
        2 \eta \left(f(x_t) - f(x^{t+1}) \right) \\ 
        &= \norm{ x_t - x^* }_2^2 - 2 \eta \left(f(x_{t+1}) - f(x^*)\right) \\ 
        &\leq \norm{ x_t - x^* }_2^2
    \end{align*}

    Where the step from 2nd to 3rd line relies on the convexity assumption and the 
    descent lemma. 
\end{proof}

\begin{remark}
    In addition to the descent lemma, contraction lemma tells us that not only the function value 
    decreases, the next step's \(x\) always gets closer to the optimum point. 
\end{remark}

% \begin{proof}
%     \begin{align*}
%         \norm{x_{t+1} x^*}^2_2 &= \norm{  (x_t - x^*) - \eta \nabla f(x_t)  }^2_2 \\
%         &= \norm{x_t - x^*}^2_2 - 2 \eta \innerProd{x_t - x^*}{\nabla f(x_t)} + \eta^2 \norm{\nablaf(x_t)}^2_2 \\ 
%         & \leq \norm{x_t - x^*}^2_2 - 2 \eta \left(f(x_t) - f(x^*)\right) + 2 \eta \left(f(x_t) - f(x_{t+1})\right) \\ 
%         &= \norm{x_t - x^*}^2_2 - 2 \eta \left(f(x_{t+1}) - f(x^*)\right) \\ 
%         &\leq \norm{x_t - x^*}^2_2
%     \end{align*}
% \end{proof}

\begin{theorem}[GD convergence for smooth convex functions]
    If \(f\) is convex and \(\beta\)-smooth, and \(\eta \leq \frac{1}{\beta}\), then: 
    \[
        f(x_t) - f(x^*) \leq \frac{2 \norm{x_0 - x^*}_2^2}{\eta t}, \ \ \forall t \geq 1   
    \]    
\end{theorem}

\begin{proof}
    Let \(\delta_t = f(x_t) - f(x^*)\). \\ 
    By the descent lemma, 
    \begin{align*}
        \delta_{t+1} \leq \delta_t - \frac{\eta}{2} \norm{\nabla f(x)}_2^2 \tag{1}
    \end{align*}

    By convexity and contraction lemma, 
    \begin{align*}
        \delta_t \leq \innerProd{\nabla f(x_t)}{x_t - x^*} 
        &\leq \norm{ \nabla f(x_t)} \norm{x_t - x^*} \\ 
        &\leq \norm{ \nabla f(x_t)} \norm{x_0 - x^*} \tag{2}
    \end{align*}

    By putting (1) and (2) together, we have 
    \[
        \delta_{t+1} \leq \delta_t - \frac{\eta}{2} \left(\frac{\delta_t}{\norm{x_0 - x^*}_2}\right)^2  
    \]

    Diving this inequality by \(\delta_t \cdot \delta_{t+1}\), we can get the following: 
    \[
        \frac{1}{\delta_t} \leq \frac{1}{\delta_{t+1}} - \frac{\eta}{2 \norm{x_0 - x^*}_2}
        \cdot \frac{\delta_t}{\delta_{t+1}} \leq \frac{1}{\delta_{t+1}} - \frac{\eta}{\norm{x_0 - x^*}_2}
    \]
    Taking the sum over \(t = 0, 1, \ldots, t - 1\): 
    \[
        \sum_{s=0}^{t-1} \left(\frac{1}{\delta_s} - \frac{1}{\delta_{s+1}} \right)
        \leq -   \frac{\eta t}{2 \norm{x_0 - x^*}_2}
    \]
    which gives to the desired inequality: 
    \[
         \delta_t \leq \frac{2 \norm{x_0 - x^*}_2^2}{\eta t}
    \]
\end{proof}

\begin{remark}
    Let \(\eta = \frac{1}{\beta}\), we get \(\delta_t \leq \frac{2 \beta \norm{x_0 - x^*}_2^2}{t}\). 

    To get \(\delta_t \leq \epsilon\), we need the step size \(t \geq \frac{2 \beta \norm{x_0 - x^*}_2^2}{\epsilon}\)
\end{remark}


\section{Convergence of GD for smooth and strongly convex functions}

\begin{definition}[strong convexity]
    \(f \colon \R^d \to \R\) is \(\alpha\)-strongly convex (\(\alpha > 0\)) if 
    \(f(x) - \frac{\alpha}{2} \norm{x}_2^2\) is convex. 
\end{definition}

\begin{lemma}[first-order \& second-order characterization of strong convexity]
    \begin{enumerate}
        \item first-order: If \\ \(f \colon \R^d \to \R\) is differentiable, then 
        \(f\) is \(\alpha\)-SC if and only if: 
        
        \[
            f(y) \geq f(x) + \innerProd{\nabla f(x)}{ y - x} + \frac{\alpha}{2} \norm{y-x}_2^2 
            \ \forall \ x, y  
        \]
        \item second-order: If \(f \colon \R^d \to \R\) is twice continuously differentiable, 
        then \(f\) is \(\alpha\)-SC if and only if: 
        \[
            \nabla^2 f(x) \succeq \alpha I, \ \forall x   
        \] 
    \end{enumerate} 
\end{lemma}

\begin{eg}
    \(f(x) = \frac{1}{2}x^T A x\) for symmetric \(A\). Then we know \(\nabla^2 f(x) = A\). 

    If \(\lambda_{\text{min}}(A) > 0\), then \(f\) is \(\lambda_{\text{min}}(A)-\text{SC}\), 
    and \(\lambda_{\text{min}}-\text{smooth}\). 
\end{eg}

\begin{theorem}[GD convergence for smooth and strongly-convex functions]
    If \(f\) is \(\beta\)-smooth and \(\alpha-\text{SC}\), and \(\alpha \leq \frac{1}{\beta}\), 
    then 
    \begin{enumerate}
        \item \(\norm{x_{t+1} - x^*}_2^2 \leq (1 - \alpha \eta) \cdot \norm{x_t - x^*}_2^2, \ \forall t\). 
        \item \(f(x_t) - f(x^*) \leq \frac{\beta}{2} (1 - \alpha \eta)^t \norm{x_0 - x^*}_2^2\). 
    \end{enumerate}
\end{theorem}


\begin{proof}
    \begin{align*}
        \norm{x_{t+1} - x^*}_2^2 &= \norm{ (x_t - x^*) - \eta \nabla f(x_t)}_2^2 \\ 
        &= \norm{ x_t - x^*}_2^2 - 2 \eta \innerProd{x_t - x^*}{\nabla f(x_t)} + \eta^2 
        \norm{ \nabla f(x_t) }_2^2 \\ 
        &\leq \norm{ x_t - x^*}_2^2 - 2 \eta \left( f(x_t) - f(x^*) + \frac{\alpha}{2}
        \norm{x_t - x^*}^2_2\right) + 2 \eta \left( f(x_t) - f(x_{t+1}) \right) \\ 
        &= (1 - \alpha \eta) \norm{x_t - x^*}_2^2 - 2 \eta \left( f(x_{t+1}) - f(x^*) \right) \\ 
        &\leq (1 - \alpha \eta)
    \end{align*}
    which finishes (1). To see how we prove (2), we proceed as follows: 

    By (1), we have \(\norm{x_t - x^*}_2^2 \leq (1 - \alpha \eta)^t \norm{x_0 - x^*}_2^2\), then: 
    \begin{align*}
        f(x_t) - f(x^*) &\leq \innerProd{\nabla f(x^*)}{x_t - x^*} + \frac{\beta}{2} \norm{x_t - x^*}_2^2 \\ 
        &\leq \frac{\beta}{2} (1 - \alpha \eta)^t \norm{x_0 - x^*}_2^2 
    \end{align*}
\end{proof}

\begin{remark}
    If \(\eta = \frac{1}{\beta}\): 
    \begin{align*}
        \norm{x_t - x^*}_2^2 &\leq \left(1 - \frac{\alpha}{\beta}\right) \norm{x_0 - x^*}_2^2 \\
        &= \left(1 - \frac{1}{\kappa}\right)  \norm{x_0 - x^*}_2^2 \\ 
        &\leq e^{- \frac{t}{\kappa}}  \norm{x_0 - x^*}_2^2
    \end{align*}

    where we set \(\kappa = \frac{\beta}{\alpha}\) to be the ``condition number'' and the last 
    line comes from \(1 - \frac{1}{\kappa} \leq e^{- \frac{1}{\kappa}}\). 

    Alternatively, if we want \( \norm{x_t - x^*}_2^2 \leq \epsilon\), we need to set 
    the step size \(t \geq \kappa \log \frac{\norm{x_0 - x^*}_2^2}{\epsilon}\). 
\end{remark}