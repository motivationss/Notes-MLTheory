
\newcommand{\hERM}{\hat{h}_{\text{ERM}}}

\chapter{Rademacher Complexity}


\section{Uniform Convergence}

Motivation: we want to study \(L(\hERM)\) and compare it against 
\(h^* \in \text{argmin}_{h \in \Hcal} L(h)\). We want to bound the 
difference \(L(\hERM) - L(h^*)\), which is also referred to as the ``\textbf{excess risk}''. 


\[
  L(\hERM) - L(h^*) = \left(L(\hERM) - L_S(\hERM)\right) + 
  \left( L_S(\hERM) - L_S(h^*) \right) + \left(L_S(h^*) - L(h^*)\right) 
\]

where the second term is smaller or equal to 0 by definition, and the third term can be bounded 
using the Hoeffding inequality as \(h^*\) does not depend on \(S\). 

Consequently, our aim becomes bounding the first term and we define the following 
\textbf{generalization gap}: 

\begin{definition}[Uniform Convergence]
    \[L(\hERM) - L_S (\hERM) \leq \sup_{h \in \Hcal} \left(L(h) - L_S(h)\right)\], 
    where the bounded difference is called the generalization gap. 
\end{definition}

\begin{theorem}[Generalization Bound for finite hypothesis class]
    If \(\Hcal\) is finite, then for any \(\delta \in (0, 1)\), we have 
    \[
        \text{w.p.} \geq 1 - \delta, \sup_{h \in \Hcal} (L(h) - L_S(h)) \leq \sqrt{\frac{\log \lvert \Hcal \rvert + \log \frac{1}{\delta}}{2n}}  
    \]
\end{theorem}


\begin{remark}
    If \(n >> \log \lvert \Hcal \rvert\), excess risk \(\to\) 0. 
\end{remark}

What if \(\Hcal\) is infinite? 

-- Idea: Reduce infinite case to finite case. 


\section{Rademacher Complexity}

\textbf{Notation}: Given \(\Hcal\) and \(\ell\), define the family of loss mappings: 
\[
    \mathcal{G} = \{ g_h \colon (x, y) \mapsto \ell (h(x), y), h \in \Hcal \}
\]

where \(z = (x, y) \sim P\), \(z_i = (x_i, y_i)\), \(\mathcal{Z} = \X \times \Y\), 
and \(L(h) = \expec_{z \sim P} \midBrac{g_h(z)}\), \(L_S(h) = \frac{1}{n} \sum_{i=1}^n g_h(z_i)\). 

\[
    \sup_{h \in \Hcal} \left(L(h) - L_S(h)\right) = \sup_{g \in \mathcal{G}} \left(
        \expec_{z \sim P} [g(z)] - \frac{1}{n} \sum_{i=1}^n g(z_i)
    \right)    
\]


\begin{definition}[Empirical Rademacher Complexity]
    Let \(\mathcal{G}\) be a set of functions mapping \(\mathcal{Z} \to \R\). 
    Let \(S = \{z_1, \cdots, z_n\} \subseteq \mathcal{Z}\). 

    The \textbf{empirical Rademacher complexity} of \(\mathcal{G}\) with respect to the simple 
    set \(S\) is: 
    \[
        R_S(\mathcal{G}) = \expec_{\sigma_1, \cdots, \sigma_n} \midBrac{\sup_{g \in \mathcal{G}} \frac{1}{n} \sum_{i=1}^n \sigma_i g(z_i)}  
    \]
    where \(\sigma_i = \begin{cases}
        +1 & \text{w.p.} \frac{1}{2} \\ 
        -1 & \text{w.p.} \frac{1}{2}
    \end{cases}\) i.i.d (called Rademacher random variables). 
\end{definition}


\begin{remark}
    Rademacher complexity measures the ability of a function class to fit random noise 
    \[
        R_S(\mathcal{G}) = \expec_{\vec{\sigma}} \midBrac{\sup_{g \in \mathcal{G}} \frac{1}{n} <\vec{\sigma}, \vec{g}_s>}  
    \]
\end{remark}


\begin{definition}[Rademacher Complexity]
    Let \(P\) be a distribution over \(\mathcal{Z}\). 
    
    For an integer \(n \geq 1\), the \textbf{Rademacher complexity} of \(\mathcal{G}\) is 
    \[
        R_n(\mathcal{G}) = \expec_{S \sim P^n} \midBrac{R_S(\mathcal{G})}  
    \]
\end{definition}


\begin{theorem}[Generalization Bound using Rademacher Complexity]
    Let \(\mathcal{G}\) be a function class mapping \(\mathcal{Z}\) to \([0,1]\), \(S = \{z_1, \ldots, z_n\} \sim P^n\). 
    Then for any \(\delta \in (0, 1)\): 

    \[
        \withp \geq 1 - \delta, \  \sup_{g\in \G} \left(\expec_{z \sim P} \midBrac{g(z)}
        - \frac{1}{n} \sum_{i=1}^n g(z_i) \right) \leq 2 R_n (\mathcal{G}) + \sqrt{\frac{\log \frac{1}{\delta}}{2n}} 
    \]

    \[
        \withp \geq 1 - \delta, \  \sup_{g\in \G} \left(\expec_{z \sim P} \midBrac{g(z)}
        - \frac{1}{n} \sum_{i=1}^n g(z_i) \right) \leq 2 R_S (\mathcal{G}) + 3\sqrt{\frac{\log \frac{2}{\delta}}{2n}} 
    \]
    
\end{theorem}


\begin{proof}
    \textbf{Step 1: Relate the sup terms to the expectation of sups using Mcdiarmid's ineq }

    Define \(f(z_1, \cdots, z_n) = \sup_{g \in \G} \left(\expec_{z \sim P}\midBrac{g(z)} - \frac{1}{n}\sum_{i=1}^n g(z_i)
    \right)\). 

    Consider \(\{z_1, \ldots, z_n\}\) and \(\{z_1', \ldots, z_n'\}\) that only differs by 
    1 point (\emph{i.e.} \(z_k \neq z_k', z_i = z_i' \ \forall i \neq k\)). 

    \begin{align*}
        f(z_1, \ldots, z_n) &= \sup_{g \in \G} \left( \expec\midBrac{g(z)} - 
        \frac{1}{n}\sum_{i=1}^n g(z_i') + \frac{1}{n}\sum_{i=1}^n g(z_i') - 
        \frac{1}{n} \sum_{i=1}^n g(z_i) \right) \\ 
        & \leq \sup_{g \in \G} \left( \expec\midBrac{g(z)} - \frac{1}{n}\sum_{i=1}^n g(z_i') \right)
        + \sup_{g \in \G} \left(\frac{1}{n} \sum_{i=1}^n g(z_i') - \frac{1}{n} \sum_{i=1}^n g(z_i)\right) \\ 
        & = f(z_1', \ldots, z_n') + \sup_{g \in \G} \left(\frac{1}{n} g(z_k') - \frac{1}{n} g(z_k)\right) \\ 
        & \leq f(z_1', \ldots, z_n') + \frac{1}{n}
    \end{align*}

    Similarly, \(f(z_i', \ldots, z_n') - f(z_1, \ldots, z_n) \leq \frac{1}{n}\). 
    Combining them we can get that \(\absolute{f(z_1, \ldots, z_n) - f(z_1', \ldots, z_n')} \leq \frac{1}{n}\). 

    Applying the Mcdiarmid's inequality, we can get the following bound: 
    \[
      \withp \geq 1 - \delta, f(z_1,\ldots, z_n) - \expec \midBrac{f(z_1,\ldots, z_n)} \leq 
      \sqrt{\frac{\log \frac{1}{\delta}}{2n}}  
    \]

    \textbf{Step 2: Bound \(\expec_S \midBrac{\sup_{g \in \G} \left(\expec_{z\sim P}\midBrac{g(z)} - 
    \frac{1}{n}\sum_{i=1}^n g(z_i)\right)}\) by Rademacher Complexity}

    Draw a fresh set of \(n\) samples \(S' = \{z_1', \ldots, z_n'\} \sim P^n\). Fix \(S\), we have 
    \begin{align*}
        \sup_{g \in \G}  \left( \expec_{z \sim P} \midBrac{g(z)} - \frac{1}{n} \sum_{i=1}^n g(z_i)\right)
        &= \sup_{g \in \G} \left( \expec_{S'} \midBrac{\frac{1}{n}\sum_{i=1}^n g(z_i)} - \frac{1}{n}\sum_{i=1}^n g(z_i)  \right) \\ 
        &= \sup_{g \in \G} \left( \expec_{S'} \midBrac{\frac{1}{n} \sum_{i=1}^n \left(g(z_i') - g(z_i)\right) }  \right) \\
        &\leq \expec_{S'} \midBrac{\sup_{g\in \G} \frac{1}{n} \sum_{i=1}^n \left(g(z_i') - g(z_i)\right)} 
    \end{align*}

    Taking expectation over \(S\) on both sides generate that 
    \begin{align*}
        \expec_S \midBrac{\sup_{g\in G} \left(\expec_{z\sim P}\midBrac{g(z)} - \frac{1}{n}\sum_{i=1}^n g(z_i)\right)}
        &\leq \expec_{S,S'} \midBrac{\sup_{g\in \G} \frac{1}{n} \sum_{i=1}^n \left(g(z_i') - g(z_i)\right)} \\ 
        &= \expec_{S, S', \vec{\sigma}} \midBrac{\sup_{g \in \G} \frac{1}{n} \sum_{i=1}^n \sigma_i 
        \left(g(z_i') - g(z_i)\right)}  \tag*{?}\\
        &\leq \expec_{S, S', \vec{\sigma}} \midBrac{\sup_{g \in \G} \frac{1}{n} \sum_{i=1}^n \sigma_i g(z_i')}
        + \expec_{S, S' \vec{\sigma}} \midBrac{\sup_{g\in\G}\frac{1}{n}\sum_{i=1}^n - \sigma_i g(z_i)} \\ 
        &= 2 R_n (\G)
    \end{align*}

    Combining the result from \textbf{step 1} and \textbf{step 2}, we prove the first inequality in the theorem. 

    \textbf{Step 3: Prove \(R_n(\G)\) and \(R_S(\G)\) are close}
    Similar to step 1, we can verify that \(R_S(\G)\) satisifies the bounded difference property. 
    
    Apply Mcdiarmid's inequality, we can get that 
    \[
      \withp \geq 1 - \delta, R_n(\G) \leq R_S (\G) + \sqrt{\frac{\log \frac{1}{\delta}}{2n}}    
    \]

    Combining the outputs from step 1 - 3 and replacing \(\delta\) with \(\delta / 2\) gives the second 
    inequality. 
\end{proof}



\chapter{VC-Dimension}

In this chapter, we only consider the binary classification case with the 0-1 loss, 
\emph{i.e.} \(y = \{ \pm 1\}\) and \(\G = \{(x, y) \mapsto \indi \midBrac{h(x)\neq y} \colon h \in \Hcal \}\).

\section{Growth Function Bounds}

\begin{lemma}\label{lem:rnG and rnH}
    \(R_n (\G) = \frac{1}{2} R_n (\Hcal)\)
\end{lemma}

\begin{proof}
    Given \(S = \{(x_i, y_i)\}_{i=1}^n\), we have 
    \begin{align*}
        R_S(\G) &= \expec_{\vec{\sigma}} \midBrac{ \sup_{h \in \Hcal} \frac{1}{n} \sigma_i \indi \midBrac{h(x_i) \neq y_i} }  \\
        &= \expec_{\vec{\sigma}} \midBrac{\sup_{h \in \Hcal} \frac{1}{n} \sum_{i=1}^n \sigma_i 
        \frac{1 - y_i h(x_i)}{2}} \\
        &= \frac{1}{2} \expec_{\vec{\sigma}} \midBrac{ \frac{1}{n} \sum_{i=1}^n \sigma_i 
        + \sup_{h \in \Hcal} \frac{1}{n} \sum_{i=1}^n \sigma_i (-y_i) h(x_i) } \\ 
        &= \frac{1}{2} \expec_{\vec{\sigma}} \midBrac{ \sup_{h \in \Hcal} \frac{1}{n} 
        \sum_{i=1}^n \sigma_i h(x_i) } \\ 
        &= \frac{1}{2} R_S(\Hcal) 
    \end{align*}
\end{proof}

\begin{remark}
    It then becomes natural to bound \(R_n (\Hcal)\). 
\end{remark}
\begin{definition}[Growth Function] \label{def:growth_function}
    The growth function \(\Pi_\Hcal \colon \N \to \N\) for a hypothesis class 
    \(\Hcal\) that maps to \(y = \{\pm 1\}\) is defined as 
    \[
        \Pi_\Hcal (n) = \sup_{x_1, \ldots, x_n \in \X} \lvert \{ \left(h(x_1),\ldots, h(x_n)\right) \colon h \in \Hcal \} \rvert  
    \]
\end{definition}

\begin{remark}
    This definition defines the set of all possible predictions on a given set of inputs. 
\end{remark}



\begin{theorem}[Generalization bound using VC-dimension]
    Let \(\Hcal\) be a hypothesis class taking values \(y = \{\pm 1\}\). Then 
    \[
        R_n (\Hcal) \leq \sqrt{ \frac{2 \log \Pi_\Hcal (n)}{n} }  
    \]
\end{theorem}

\begin{proof}
    Let \(S = \{x_1, \ldots, x_n\}\), \(Q = Q_S = \{\left(h(x_1), \ldots, h(x_n) \colon h \in \Hcal \right)\}\). 

    We want to show that \(R_S(\Hcal) \leq \sqrt{\frac{2\log \absolute{Q}}{n}}\) 

    \begin{align*}
        R_S (\Hcal) &= \expec_{\vec{\sigma}} \midBrac{ \sup_{h\in \Hcal} \frac{1}{n} \sum_{i=1}^n \sigma_i h(x_i) } \\ 
                    &= \expec_{\vec{\sigma}} \midBrac{ \sup_{\vec{v} \in Q} \sampleAverage \sigma_i v_i } \tag*{Apply Hoeffding} 
    \end{align*}

    Then for all \(\lambda > 0\),  

    \begin{align*}
        e^{\lambda R_S(\Hcal)} &= e^{  \lambda \expec_{\vec{\sigma}} \midBrac{ \sup_{\vec{v} \in Q} \sampleAverage \sigma_i v_i }  } \\
                               &\leq \expec_{\vec{\sigma}} \midBrac{  e^{\lambda \sup_{\vec{v} \in Q} \sampleAverage \sigma_i v_i } } \tag*{Jensen's ineq} \\ 
                               & \leq \expec_{\vec{\sigma}} \midBrac{ \sum_{\vec{v} \in Q} e^{\lambda \sampleAverage \sigma_i v_i }} \tag*{\color{red}why?} \\ 
                               & = \sum_{\vec{v} \in Q} \expec_{\vec{\sigma}} \midBrac{ e^{\lambda \sampleAverage \sigma_i v_i} } \\ 
                               & \leq \sum_{\vec{v} \in Q} e^{\frac{\lambda^2}{2n}} \tag*{by Hoeffding} \\ 
                               &= \absolute{Q} e^{\frac{\lambda^2}{2n}}
    \end{align*}
    This gives that \(R_S(\Hcal) \leq \frac{1}{\lambda} \log \absolute{Q} + \frac{\lambda}{2n}\) 

    Choose \(\lambda = \sqrt{2n \log \absolute{Q}}\) and we can get that \(R_S(\Hcal) \leq \sqrt{\frac{2\log \absolute{Q}}{n}}\)
\end{proof}

\begin{remark}
    Discussions about the growth function: 
    \begin{itemize}
        \item When \(\Hcal\) is finite, we have that \(\Pi_{\Hcal} (n) \leq \absolute{\Hcal}\) 
        \[
            R_n (\Hcal) \leq \sqrt{ \frac{2\log \absolute{\Hcal}}{n} } \text{ recovers Thm 1}  
        \]
        \item When \(\Hcal\) is ``super power'', \(\Pi_\Hcal (n) = 2^n\), \emph{i.e. overfitting}. 
        \[
            R_n (\Hcal) \leq \sqrt{ \frac{2 \log 2^n}{n}} = \sqrt{2 \log 2}  
        \]
        \item What if the growth function is in-between, a polynomial function? 
        
        Suppose \(\Pi_\Hcal (n) \leq n^d\), we have that 
        \[
            R_n(\Hcal) \leq \sqrt{ \frac{2 d \log n}{n}} \to 0 \text{ if } n >> d \log d 
        \]
    \end{itemize}
\end{remark}




\begin{definition}[VC-dimension]\label{def:VC_dimension}
    The VC-dimension of a class of hypothesis function \(\Hcal\) is 
    \[
        \text{VC}(\Hcal) = \max \{ n \colon \Pi_\Hcal (n) = 2^n \}  
    \]
\end{definition}

\begin{definition}[Shatter]\label{def:shatter}
    \(S = \{x_1, \ldots, x_n\}\) can be shattered by \(\Hcal\) if 
    \(\forall \ y_1, \ldots, y_n \in \{\pm 1\}, \exists h \in \Hcal \text{s.t.} h(x_i)=y_i\)
    for all \(i = \{1, \ldots, n\}\). 
\end{definition}

\begin{remark}
    The VC-dimension is the maximum size of a sample set \(S\) that can be \textbf{shattered}
    by \(\Hcal\). 
\end{remark}

\begin{eg}[Threshold Function]
    Let \(\X = \R\),\(\Hcal = \{h_a \colon a\in \R\}\),
     \(h_a \in \Hcal\), \(h_a (x) = \begin{cases}
        +1, &\text{if} \ x \geq a \\ 
        -1, &\text{if} \ x < a 
    \end{cases}\)

     \begin{center}
        Then \(\mathbf{VC-dim}(\Hcal) = 1\)
     \end{center} 
\end{eg}

\begin{proof}
    \begin{enumerate}
        \item any input \(x \in \R\) can be shattered 
        \[
            h_{x-1}(x) = +1, \ \ h_{x+1} = -1 
        \]
        \item any inputs \(x_1, x_2 \in \R\) cannot be shattered 
        \[
            x_1 \leq x_2, \ \text{impossible to label } (+1, -1)  
        \]
    \end{enumerate}
\end{proof}



\begin{theorem}[growth function bound]
    Let \(\Hcal\) be a hypothesis class with VC-dimension \(d\). Then, 
    \[
        \forall \ n >> d \colon \Pi_\Hcal (n) << \left(\frac{e^n}{d}\right)^d \leq n^d \ \ \text{if } d \geq 3  
    \]
\end{theorem}


\begin{theorem}[Generalization Bound Using VC-Dimension]
    Let \(\Hcal\) be a hypothesis class taking values in \(y = \{\pm 1\}\) and has 
    VC-dim \(d\). Consider the 0-1 loss. 

    Then, for all \(\delta \in (0, 1)\), 
    \[
    \withp \geq 1 - \delta, \sup_{h \in \Hcal} \left(L(h) - L_S(h)\right) \leq 
    \sqrt{\frac{2d \log e^n}{d}} + \sqrt{\frac{\log \frac{1}{\delta}}{2n}}  
    \]
\end{theorem}

\begin{remark}
    This VC-dimension bound requires \(n >> d\). In other words, it is effective when the 
    hypothesis class is relatively less expressive. 
\end{remark}