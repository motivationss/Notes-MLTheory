\section{More on VC-Dimension}
First we look at more examples illustrating the concept of VC-dimension. 

\begin{eg}[Axis-aligned rectangles]
    Let \(\X = \R\), \(\Hcal = \{ h_{a,b,c,d} \colon a,b,c,d \in \R \}\),  and \(h \in \Hcal\) be the form 
    \[
        h_{a,b,c,d} (x) = \begin{cases}
            1 & \text{ if } x_1 \in [a,b], x_2 \in [c, d] \\ 
            -1 & \text{ otherwise}
        \end{cases}  
    \]
    
    Then we have \textbf{Vc-dim} \(\Hcal = 4\). 
\end{eg}

\begin{proof}
    \begin{enumerate}
        \item there exists 4 points that can be shattered  \color{red}{exists or for all}? 
        
        \item Any 5 points cannot be shattered  \\ 
        Choose the minimum axis-aligned rectangle that contains all 5 points, then it is impossible 
        to label the sides +1 while labeling inside one -1  
    \end{enumerate}
\end{proof}

\begin{eg}[Linear Functions]
    Let \(\X = \R\), \(\Hcal = \{h_w \colon w \in \R^d\}\), and 
    \[h_w(x) = \text{sign}(w^Tx)
    = \begin{cases}
        1 &\text{if } w^Tx \geq 0 \\ 
        -1 &\text{if } w^tx < 0 
    \end{cases}\]
    Then \textbf{Vc-dim}(\(\Hcal\)) = \(d\). 
\end{eg}

\begin{proof}
    \begin{enumerate}
        \item \(\exists \ d\) points that can be shattered \color{red} Same Question exists or for all?
        
        \color{black} Choose \(x_1, \ldots, x_d \in \R^d\) that are linearly independent. \\ 
        Then for all \(y_1, \ldots, y_d \in \{\pm 1\}\), we can find a \(w \in \R^d\) such that 
        \(w^T x_i = y_i\), for all \(i=1,\ldots, d\) by solving the set of linear equations. 
        
        \item Any \(d+1\) point cannot be shattered  
        
        Assume for the sake of contradiction that there exists \(d+1 \ \text{points} \colon 
        x_1, \ldots, x_d\) that can be shattered. \\ 
        In formal terms, \(\exists \  \alpha = (\alpha_1, \ldots, \alpha_{d+1})  \ \text{s.t.} 
        \sum_{i=1}^{d+1} \alpha_i x_i = 0,\  \alpha \neq 0\), \emph{i.e.} \(\exists \ 
        \text{a coordinate} \ k \in \{ 1,\ldots, d+1\} \ \text{s.t.} \ \alpha_k \neq 0\). 
        WLOG we can assume \(\alpha_k > 0\).  
        
        For all \(w \in \R^d\), we must have 
        \(\sum_{i=1}^{d+1} \alpha_i w^T x_i = 0\).  \color{red} why? 

        \color{black} Then let \(y_i = \text{sign}(\alpha_i), i=1,\ldots,d+1\). \(\exists w
        \in \R^d \ \text{s.t.} \ \text{sign}(w^T x_i) = y_i\). 

        Then we find the contradiction: 
        \[
            0 = \sum_{i=1}^{d+1} \alpha_i (w^T x_i) < 0 \tag*{opposite sign}
        \]
    \end{enumerate}
\end{proof}


\begin{eg}[Sine Function]
    Let \(\X = \R\), \(\Hcal = \{h_\omega \colon \omega \in \R\}\), and 
    \(h = \text{sign}\left(\sin(\omega x)\right)\)

    Then \textbf{Vc-dim}( \(\Hcal\)) = \(\infty\). 
\end{eg}

\begin{proof}
    It suffices to show that \(\exists\) n points that can be shattered, for any \(n\). \\ 
    Consider \(n\) points, \(x_i = 2^{-i}\) (\(i = 1, \ldots, n\)) and any labeling 
    \(y_1, \ldots, y_n \in \{\pm 1\}\). 

    Define \( \frac{\omega}{\pi} = \left( y_n' y_{n-1}' \ldots y_1' 1 \right)_2\) in terms of 
    binary integer, where \(y_i' = \begin{cases}
        0 &\text{if } y_i = 1 \\ 
        1 &\text{if } y_i = -1 
    \end{cases}\) 

    WTS \(\text{sign}\left(\sin (\omega x_i)\right) = y_i\), 

    which can be realized through 
    \[
        \frac{\omega x_i}{\pi} = \frac{\omega}{\pi} 2^{-i} = \left(y_n' y_{n-1}' \ldots y_1' 1 \right)_2 
    \]

    \color{red} Not fully understand 
\end{proof}


\begin{theorem}[VC-dimension in finite precision]
    Let \(\Hcal\) be parametrized by \(p\) parameters, with each stored in \(k\) bits. 
    \(\Hcal = \{h_\theta, \theta \in \R^P\}\), then VC-dim(\(\Hcal\)) \(\leq k \cdot p\). 
\end{theorem}

\begin{proof}
    There are \((2^k)^p\) choices for \(\theta = (\theta_1, \ldots, \theta_p)\), and then 
    \[
        2^{\text{Vc-dim}(\Hcal)} \leq \absolute{\Hcal} \leq 2^{kp}
    \]
\end{proof}

\begin{remark}[Limitation of VC-dimension]
    \begin{align*}
        L(h) - L_S(h) &\leq \tilde{O} \left(\sqrt{\frac{VC-dim(\Hcal)}{n}}\right) \\ 
                      &\leq \tilde{O} \left(\sqrt{\frac{\#\text{params}}{n}}\right)
    \end{align*}

    If \# params >> \# samples, the bound will become vacuous. 
\end{remark}


\chapter{Margin Theory}

We focus on the binary classification setting where \(y = \{\pm 1\}\). 

\section{Basic Setups}

\begin{definition}[Margin]\label{def:margin}
    The margin of a function \(h \colon \X \to \R\) at a point \(x \in \X\) labeled 
    with \(y \in \{\pm 1\}\) is \(yh(x)\). 
\end{definition}

\begin{remark}
    We have \( \hat{y} = \text{sign}\left(h(x)\right)\); and a classification is correct when 
    \(y h(x) > 0\). 
\end{remark}

\begin{definition}[Margin Loss]\label{def:margin_loss}
    For any \(\gamma > 0\), define \(\gamma\)-margin loss as 
    \[
        \ell_\gamma (y', y) = \ell_\gamma(yy') = \begin{cases}
            1, &\text{if } yy' \leq 0 \\ 
            1 - \frac{yy'}{\gamma} &\text{if } 0 < yy' < \gamma \\ 
            0, &\text{if } yy' \geq \gamma 
        \end{cases}  
    \]
\end{definition}

\begin{remark}
    Margin Loss \(\geq\) 0-1 loss (in terms of their graphs). 
\end{remark}

\begin{definition}[Population \& Empirical Risk for Margin Loss]
    \[
        L_\gamma (h) = \expec_{(x,y) \sim P} \midBrac{\ell_\gamma \left(h(x), y\right)} 
    \] 
    \[ 
        L_{\gamma, S} (h) = \frac{1}{n} \sum_{i=1}^n \ell_\gamma \left(h(x_i), y_i\right)  
    \]
\end{definition}

\begin{remark}
    \(\ell_\gamma (\cdot)\) is \(\frac{1}{\gamma}\)-Lipschitz. 
\end{remark}

SideNote: We say \(f \colon \R \to R\) is \(C-\)Lipschitz if \(\absolute{f(x) - f(x')}
\leq C\absolute{x - x'}\) for all \(x, x' \in \R\). OR equivalently, \(\absolute{f'(x)} \leq C,
\forall x \in R\). 

\begin{lemma}[Talagrend's Lemma]
    Let \(\phi \colon \R \to \R\) be a C-Lipschitz function. Then, 
    \[
        R_S (\phi \ \circ \ \Hcal) \leq C \cdot R_S(\Hcal)  
    \]
    where \(\phi \ \circ \ \Hcal = \{z \mapsto \phi(h(z)) \colon h \in \Hcal\}\).  
\end{lemma}

\begin{theorem}[Margin-based generalization bound for binary classificaiton]
    Let \(\Hcal\) be a function class mapping \(\X \to \R\). Fix \(\gamma > 0\). Then, 
    for any \(\delta \in (0,1)\), with probability \(\geq 1 - \delta\) we have: 
    \[
        \sup_{h \in \Hcal} \left( L_\gamma (h) - L_{\gamma, S} (h) \right)  
        \leq \frac{2}{\gamma} R_n(\Hcal) + \sqrt{ \frac{\log \frac{1}{\delta}}{2n} }  
    \]

    Also with probability \(\geq 1 - \delta\), we have: 

    \[
        \sup_{h \in \Hcal} \left( L_\gamma (h) - L_{\gamma, S} (h) \right)  
        \leq \frac{2}{\gamma} R_S(\Hcal) + 3\sqrt{ \frac{\log \frac{1}{\delta}}{2n} }  
    \]
\end{theorem}


\begin{proof}
    \begin{align*}
        G_\gamma &= \{ (x,y) \mapsto \ell_\gamma (yh(x)) \colon h \in \Hcal \} \\ 
                 &= \{ (x,y) \mapsto \ell_\gamma (\hat{h}(x, y)) \colon \hat{h} \in \hat{\Hcal}  \} \\ 
                 &= \ell_\gamma \circ \hat{\Hcal}
    \end{align*}
    where \(\hat{\Hcal} = \{ (x,y) \to yh(x) \colon h \in \Hcal \}\). 
    \begin{align*}
        R_S(\hat{\Hcal}) &= \expec_{\vec{\sigma}} \midBrac{\sup_{h \in \Hcal} \sampleAverage 
        \sigma_i y_i h(x_i)} \\ 
                         &= \expec_{\vec{\sigma}} \midBrac{\sup_{h \in \Hcal} \sampleAverage 
                         \sigma_i h(x_i)} \\ 
                         &= R_S (\Hcal)
    \end{align*}

    By Talagrend's lemma, \(R_S(G_\gamma) \leq \frac{1}{\gamma} R_S(\hat{\Hcal}) = \frac{1}{\gamma}
    R_S(\Hcal)\). 

    Completes the proof by applying the generalization bound for \(G_\gamma\) 
    

    \color{red} What generalization bound?
\end{proof}


\begin{remark}
    
    \[
        L_{0-1} (h) \leq L_\gamma (h) \leq L_{\gamma, S} (h) + \frac{2}{\gamma} R_n (\Hcal) + 
        \sqrt{ \frac{\log \frac{1}{\delta}}{2n}  }  
    \]

    Tradeoff: As \(\gamma\) increases, \(L_{\gamma, S} (h)\) increases but 
    \(\frac{2}{\gamma} R_n (\Hcal)\) decreases. 
\end{remark}

Assume that \(h\) perfectly classifies the dataset \(S\): 

\[
    y_i h(x_i) > 0, \ \forall i = 1, \ldots, n \ \ \ \ L_{0-1, S} (h) = 0 
\]

Let \(\gamma_{S,h} = \min_{i \in [n]} y_i h(x_i)\), which is the maximum \(\gamma\) 
such that \(L_{\gamma, S}(h) = 0\). Then: 

\[
    L_{0-1} (h) \leq L_\gamma (h) \leq  \frac{2}{\gamma_{S,h}} R_n (\Hcal) + 
     \sqrt{ \frac{\log \frac{1}{\delta}}{2n}  }  
\]



\section{Margin Bound for Linear functions}

\begin{theorem}[Rademacher complexity of linear functions with bounded weights]
    Let \(\Hcal = \{ x \mapsto  w^T x \colon w \in \R^d, \norm{w}_2 \leq B\}\) and 
    
\end{theorem}